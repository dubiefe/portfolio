{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114912,"databundleVersionId":13710467,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Emilie Dubief","metadata":{}},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"Image Classification is a technique used to sort images between labels depending on its pixels. It can have multiple applications, such as the one used in this notebook: Binary Classification. This consists of sorting images into two categories.\nTo do this kind of classification, several methods can be used, like CNN (Convolutional Neural Network). This method is an algorithm of deep learning that learns patterns from data and applies them to other data to make predictions.\nAnother algorithm used for image classification in Transfer Learning that consists of using an already existing CNN to improve another.","metadata":{}},{"cell_type":"markdown","source":"Before entering more into the details for this notebook, let's recap what the objectives are.","metadata":{}},{"cell_type":"markdown","source":"For the assignment, the objective was to create a CNN model that could do binary classification between two categories: cats and dogs.\nIn order to build the model, we had an already existing dataset filled with pictures taken by former students of their pets.\nBy looking into the data set, I noticed that there was a wide array of diversity in the pictures. In fact, many ones were uncanny pictures of the cats and dogs among more \"classic\" ones. Which already gives a good representation of what cats and dogs pictures can be. ","metadata":{}},{"cell_type":"markdown","source":"In this notebook, I will use this dataset to build a CNN model. First, you will see the methodology I used to build the model, then the results I had all along the process of building the model. Finally, you will find the conclusions of this experiment along with the references I used in this notebook.","metadata":{}},{"cell_type":"markdown","source":"# Methodology","metadata":{}},{"cell_type":"markdown","source":"In this section, you can find the final model build to do a binary classification between cat and dog pictures. If you want to run this notebook, be aware that it takes more than 2 hours in Kaggle.","metadata":{}},{"cell_type":"markdown","source":"## Basics imports","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom tensorflow import data as tf_data\nimport keras\n\nfrom tensorflow.keras import layers\n\nseed = 42\nkeras.utils.set_random_seed(seed)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:09:32.168376Z","iopub.execute_input":"2025-10-24T10:09:32.168699Z","iopub.status.idle":"2025-10-24T10:09:36.036622Z","shell.execute_reply.started":"2025-10-24T10:09:32.168673Z","shell.execute_reply":"2025-10-24T10:09:36.036041Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Read in the training data","metadata":{}},{"cell_type":"markdown","source":"In this section, I read the dataset consisting of all the pictures of cats and dogs of former students.\n\nI enlarged this dataset by flipping and rotating the images.","metadata":{}},{"cell_type":"code","source":"image_size = (256, 256)\n\n# when working with 20_000 files for training this\n# will lead to exactly 160 mini-batches per epoch\nbatch_size = 125\n\n# https://keras.io/api/data_loading/image/#imagedatasetfromdirectory-function\ntrain_ds, val_ds = keras.utils.image_dataset_from_directory(\n    #\"PetImages\",\n    \"/kaggle/input/u-tad-dogs-vs-cats-2025/train/train\",\n    validation_split=0.2,\n    subset=\"both\",\n    seed=seed,\n    image_size=image_size,\n    batch_size=batch_size,\n    labels=\"inferred\",\n    label_mode=\"categorical\",\n)\n\n# Adding more data by tranforming randomly the data\naugmentation_layers = [\n    layers.RandomFlip(\"horizontal\"), # horizontal flip\n    layers.RandomRotation(0.1), # rotation \n]\n\ndef data_augmentation(x):\n    for layer in augmentation_layers:\n        x = layer(x)\n    return x\n\n# Transform the training data\ntrain_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:09:36.037960Z","iopub.execute_input":"2025-10-24T10:09:36.038497Z","iopub.status.idle":"2025-10-24T10:09:51.406111Z","shell.execute_reply.started":"2025-10-24T10:09:36.038477Z","shell.execute_reply":"2025-10-24T10:09:51.405527Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Transfer learning","metadata":{}},{"cell_type":"markdown","source":"In this section, you can see the implementation of the transfer learning. The idea here is to take an already trained CNN model to improve the results of mine.\n\nThis implementation was made thanks to the Keras documentation present in the references section.\n\nFirst, I needed to create a base model from ImageNet.\n\nThen, I froze this model so that it won't be trained along with my model.\n\nFinally, I created a new model on top of the base model.","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\n\nbase_model = keras.applications.Xception(\n    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n    input_shape=(256, 256, 3), # fit the images to the same size\n    include_top=False,\n)\n\n# Freeze the base_model\nbase_model.trainable = False\n\n# Create new model on top\nmodel = Sequential([\n    base_model,\n    keras.layers.GlobalAveragePooling2D(),\n    keras.layers.Dense(512, activation='relu'),\n    keras.layers.Dense(2, activation='softmax')\n])\n\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:09:51.406991Z","iopub.execute_input":"2025-10-24T10:09:51.407272Z","iopub.status.idle":"2025-10-24T10:09:53.199376Z","shell.execute_reply.started":"2025-10-24T10:09:51.407250Z","shell.execute_reply":"2025-10-24T10:09:53.198760Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compile and train (fit)","metadata":{}},{"cell_type":"markdown","source":"In this section, I compiled the model with the optimizer Adam and I trained the model using the data from the dataset.","metadata":{}},{"cell_type":"code","source":"%%time\n\nmodel.compile(optimizer=keras.optimizers.Adam(1e-5),  # Very low learning rate\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nepochs = 40\n\nprint(\"Training the top layer\")\nhistory = model.fit(train_ds,\n                    validation_data = val_ds,\n                    epochs = epochs,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:09:53.200732Z","iopub.execute_input":"2025-10-24T10:09:53.200914Z","execution_failed":"2025-10-24T10:10:31.383Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fine-tuning","metadata":{}},{"cell_type":"markdown","source":"In this section, I did some fine tuning to improve the model I trained previously.\n\nThe first step is to defroze the base model to actually train it along with my model.\n\nThen, I recompiled the model with the new data that I defrozed.\n\nFinally, I can fit the model with the base model taken in account.","metadata":{}},{"cell_type":"code","source":"# Unfreeze the base_model. Note that it keeps running in inference mode\n# since we passed `training=False` when calling it. This means that\n# the batchnorm layers will not update their batch statistics.\n# This prevents the batchnorm layers from undoing all the training\n# we've done so far.\nbase_model.trainable = True\nmodel.summary(show_trainable=True)\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n    metrics=[keras.metrics.BinaryAccuracy()],\n)\n\nepochs = 1\nprint(\"Fitting the end-to-end model\")\nmodel.fit(train_ds, validation_data=val_ds, epochs=epochs,)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-24T10:10:31.383Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plot the learning curves","metadata":{}},{"cell_type":"markdown","source":"In this section, you can observe the learning curves related to my model; you'll have to launch the notebook to actually see them.","metadata":{}},{"cell_type":"code","source":"logs = pd.DataFrame(history.history)\n\nplt.figure(figsize=(14, 4))\nplt.subplot(1, 2, 1)\nplt.plot(logs.loc[1:,\"loss\"], lw=2, label='training loss')\nplt.plot(logs.loc[1:,\"val_loss\"], lw=2, label='validation loss')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.plot(logs.loc[1:,\"accuracy\"], lw=2, label='training accuracy')\nplt.plot(logs.loc[1:,\"val_accuracy\"], lw=2, label='validation accuracy')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend(loc='lower right')\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-24T10:10:31.383Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save the trained model","metadata":{}},{"cell_type":"code","source":"model.save(\"model.keras\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-24T10:10:31.383Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate model performance using the `supplementary_data`","metadata":{}},{"cell_type":"code","source":"supplementary_ds = keras.utils.image_dataset_from_directory(\n    #\"PetImages\",\n    \"/kaggle/input/u-tad-dogs-vs-cats-2025/supplementary_data/supplementary_data\",\n    image_size=image_size,\n    batch_size=batch_size,\n    labels=\"inferred\",\n    label_mode=\"categorical\",\n)\n\nmodel.evaluate(supplementary_ds,\n               return_dict=True,\n               verbose=1)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-24T10:10:31.384Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Results\n\n## Details of trials\n\nTo finally arrive to this model, I tried a lot of different things detailed below along with the accuracy score associated with each trial.\n\nThe first step was to launch the template model for the competition, consisting of a basic CNN model, resulting in an accuracy score of 0.65.\n\nMy first change in the building of the model was to use another optimizer: Adam instead of RMSprops. I found out in the example notebook shown in class that it was one of the best optimizers when I did my tests. Yet this change only reduced my accuracy score by giving me 0.61. I decided to keep it to see what it would do with my next updates.\n\nThen, I looked into the Keras documentation to do some transfer learning and fine tuning to my model. The idea was to use an already trained model to better mine. I added some pre-trained models from ImageNet on top of my model. I froze that model to not use it during the training. After that I defroze it and do some fine tuning of my model with it. This increased my score a lot with an accuracy of 0.78.\n\nIn order to train my model more, I added some epochs, not much at first because it took a long time for my model to run. I added 5 epochs and that added 0.01 to my accuracy score.\n\nYet, my priority was first to add more parameters to train my model, I would then add more epochs to train it as much as possible.\n\nThe next step was to add more data by transforming the one I already had. In fact, the dataset doesn't have a lot of data; it was necessary to add more in order to hope for a better accuracy. To do so I started by flipping horizontally my dataset to have more data to train on. This added 0.02 to my accuracy score, which was then 0.81.\n\nThanks to the Keras documentation and the courses, I saw that I also could rotate the images of my dataset to add even more data to my model, I added a random rotation of 0.1. This update increased my accuracy score to 0.89.\n\nFinally, I started adding epochs to train even more my model. I didn't do it a lot before all my changes because it would already take 25 minutes to run. I added epochs incrementally (5 by 5) to finally end up with 40 epochs and an accuracy of 0.91. ","metadata":{}},{"cell_type":"markdown","source":"## Create predictions for all of the test images\n(Do not modify this section)","metadata":{}},{"cell_type":"code","source":"%%time\n\nfolder_path = \"/kaggle/input/u-tad-dogs-vs-cats-2025/test/test\"\n\npredictions_dict = {}\n\nfor img in os.listdir(folder_path):\n    img = os.path.join(folder_path, img)\n    \n    # save the image name\n    file_name = img.split('/')[-1]\n    file_no_extension = file_name.split('.')[0]\n    \n    img = keras.utils.load_img(img, target_size=image_size)\n    img_array = keras.utils.img_to_array(img)\n    img_array = keras.ops.expand_dims(img_array, 0)\n    prediction = model.predict(img_array, verbose=None)\n    label = np.argmax(prediction)\n\n    # save the predictions to a dictionary\n    predictions_dict[int(file_no_extension)] = label","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-24T10:10:31.384Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save your predictions to a competition submission file","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame(predictions_dict.items(), columns=[\"id\", \"label\"]).sort_values(by='id', ascending=True)\nsubmission.to_csv('submission.csv',index=False)\n\n# print numbers of each class label\nsubmission[\"label\"].value_counts()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-24T10:10:31.384Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusions","metadata":{}},{"cell_type":"markdown","source":"The experiment allowed me to better my understanding of image classification and deep learning. It is really interesting to see how technical it is for a machine to do something so simple for most of the humans.\n\nMy final model resulted in using deep learning along with transfer learning. I think I could have improved it more, maybe by adding more epochs or even by researching more about the parameters of all the functions I used. Yet, I am really satisfied with the accuracy score I end with (0.91).\n\nThe dataset we had was also really small, so it was hard in the end to really improve significantly the score, I think transfer learning was really the key, maybe other datasets online were better than the one I used. Thus, with a larger initial dataset and another one for transfer learning, I think the accuracy score could be even more improved.\n\nThis assignment was really interesting to understand all the mechanisms of image classification and how complex it is for only binary classification. Therefore, I can only imagine how complex it is for multiple classifications with more complex images.","metadata":{}},{"cell_type":"markdown","source":"# References\n\nKeras documentation about transfer learning and fine-tuning: \nhttps://keras.io/guides/transfer_learning/ \n\nGeeks for geeks article about Image Classification : \nhttps://www.geeksforgeeks.org/computer-vision/what-is-image-classification/","metadata":{}}]}